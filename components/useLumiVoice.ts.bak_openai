"use client";

import { useEffect, useMemo, useRef, useState } from "react";

export type VoiceStatus = "idle" | "speaking" | "paused";

/**
 * Break text into conversational chunks.
 * Shorter = more human.
 */
function chunkText(text: string) {
  const cleaned = text.replace(/\s+/g, " ").trim();
  if (!cleaned) return [];

  // Split on natural pauses
  const parts = cleaned.split(/(?<=[.!?])\s+|—|–/);

  const chunks: string[] = [];
  let buffer = "";

  for (const p of parts) {
    const next = (buffer + " " + p).trim();

    if (next.length <= 90) {
      buffer = next;
    } else {
      if (buffer) chunks.push(buffer);
      buffer = p.trim();
    }
  }

  if (buffer) chunks.push(buffer);

  return chunks;
}

export default function useLumiVoice() {
  const [voices, setVoices] = useState<SpeechSynthesisVoice[]>([]);
  const [status, setStatus] = useState<VoiceStatus>("idle");
  const [enabled, setEnabled] = useState(true);

  const queueRef = useRef<SpeechSynthesisUtterance[]>([]);

  useEffect(() => {
    if (typeof window === "undefined" || !("speechSynthesis" in window)) return;

    function load() {
      setVoices(window.speechSynthesis.getVoices());
    }

    load();
    window.speechSynthesis.onvoiceschanged = load;

    return () => {
      window.speechSynthesis.onvoiceschanged = null;
    };
  }, []);

  const bestVoice = useMemo(() => {
    if (!voices.length) return null;

    const preferred = [
      "Samantha",
      "Ava",
      "Victoria",
      "Allison",
      "Google US English",
      "Google UK English Female",
      "Microsoft Aria Online",
      "Microsoft Jenny Online",
    ];

    const avoid = /robot|zira|fred|male|compact/i;

    const candidates = voices
      .filter((v) => v.lang?.startsWith("en"))
      .filter((v) => !avoid.test(v.name))
      .sort((a, b) => Number(b.localService) - Number(a.localService));

    for (const name of preferred) {
      const found = candidates.find((v) => v.name === name);
      if (found) return found;
    }

    return candidates[0] || voices[0];
  }, [voices]);

  function stop() {
    if (!("speechSynthesis" in window)) return;
    window.speechSynthesis.cancel();
    queueRef.current = [];
    setStatus("idle");
  }

  function speak(text: string) {
    if (!enabled || !("speechSynthesis" in window)) return;

    const chunks = chunkText(text);
    if (!chunks.length) return;

    stop();

    // Gentle lead-in (this reduces robotic onset)
    const lead = new SpeechSynthesisUtterance("…");
    lead.volume = 0;
    window.speechSynthesis.speak(lead);

    const utterances = chunks.map((chunk, idx) => {
      const u = new SpeechSynthesisUtterance(chunk);
      if (bestVoice) u.voice = bestVoice;

      // Natural human ranges
      u.rate = 0.88 + Math.random() * 0.06;   // subtle variation
      u.pitch = 1.02 + Math.random() * 0.04;  // warmth without chipmunk
      u.volume = 1;

      u.onstart = () => setStatus("speaking");

      u.onend = () => {
        // Add a micro pause between thoughts
        if (idx < chunks.length - 1) {
          const pause = new SpeechSynthesisUtterance(" ");
          pause.volume = 0;
          pause.rate = 0.6;
          window.speechSynthesis.speak(pause);
        } else {
          setStatus("idle");
        }
      };

      u.onerror = () => setStatus("idle");

      return u;
    });

    queueRef.current = utterances;
    utterances.forEach((u) => window.speechSynthesis.speak(u));
  }

  function pause() {
    if (!("speechSynthesis" in window)) return;
    window.speechSynthesis.pause();
    setStatus("paused");
  }

  function resume() {
    if (!("speechSynthesis" in window)) return;
    window.speechSynthesis.resume();
    setStatus("speaking");
  }

  return {
    supported: typeof window !== "undefined" && "speechSynthesis" in window,
    status,
    isSpeaking: status === "speaking",
    enabled,
    setEnabled,
    speak,
    stop,
    pause,
    resume,
    voiceName: bestVoice?.name ?? null,
  };
}
